# -*- coding: utf-8 -*-
"""Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SaYN5H9b373CtUaxV0N3stk-FQx2R6T0

<br>
<font>
<!-- <img src="https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png" alt="SUT logo" width=300 height=300 align=left class="saturate"> -->
<div dir=ltr align=center>
<img src="https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png" width=200 height=200>
<br>
<font color=0F5298 size=7>
Machine Learning <br>
<font color=2565AE size=5>
Electrical Engineering Department <br>
Spring 2024<br>
<font color=3C99D size=5>
Practical Assignment 4 <br>
<font color=696880 size=4>
<!-- <br> -->


____

# Personal Data
"""

student_number = '99101643'
first_name = 'Sara'
last_name = 'Rezanezhad'

"""# Introduction

In this assignment, we will be performing clustering on Spotify songs.

# Data Preprocessing

In the next cell, import the libraries you'll need.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import random
from sklearn.decomposition import PCA
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import warnings

"""In the `spotify.csv` file, load the data. Exclude unrelated features and retain only the track name and the features you believe are relevant."""

data = pd.read_csv('spotify.csv')
data.head(10)

"""In this cell, you should implement a standard scalar function from scratch and applying it to your data. Explian importance behind using a standard scalar and the potential complications that could arise in clustering if it's not employed. (you can't use `sklearn.preprocessing.StandardScaler` but you are free to use `sklearn.preprocessing.LabelEncoder`)"""

def standard_scaler(X):
    """
    Standardize the numerical features by removing the mean and scaling to unit variance.

    Parameters:
    X (pd.DataFrame): The input data to be standardized.

    Returns:
    pd.DataFrame: The standardized data.
    """
    # Select only the numerical columns
    num_cols = X.select_dtypes(include=['int64', 'float64']).columns
    X_num = X[num_cols]

    # Calculate the mean and standard deviation of each numerical feature
    means = X_num.mean()
    stds = X_num.std()

    # Standardize the numerical data
    X_num_std = (X_num - means) / stds

    # Create a new DataFrame with the standardized numerical columns and the original non-numerical columns
    X_std = X.copy()
    X_std[num_cols] = X_num_std

    return X_std

# Apply the standard scalar to the data
data_std = standard_scaler(data)

print(f'shape of data: {data.shape}\n')

print(f'information of data:\n{data.info()}\n')

print(f'summary statistics of the data:\n{data.describe()}\n')

# Removing unnecesary columns

data=data.drop(['track_id','track_name','track_album_id'
,'track_album_name','track_album_release_date','playlist_name','playlist_id','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence'
,'tempo','duration_ms'],axis=1)

# Replacing any NaN values in the dataframe with 0
data = data.fillna(value=0)

print(f'number of NaN values in each column:\n{data.isna().sum()}')

# Creating a pairplot using Seaborn library
sns.pairplot(data)

"""# Dimensionality Reduction

One method for dimensionality reduction is Principal Component Analysis (PCA). Use its implementation from the `sklearn` library to reduce the dimensions of your data. Then, by using an appropriate cut-off for the `_explained_variance_ratio_` in the PCA algorithm, determine the number of principal components to retain.
"""

# Select only the numeric columns
data_std = data_std.select_dtypes(include=['int64', 'float64'])

# Perform PCA
pca = PCA()
X_pca = pca.fit_transform(data_std)

# Determine the number of principal components to retain
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)

# Choose the number of principal components based on a suitable cut-off for the explained variance ratio
n_components = next(i for i, v in enumerate(cumulative_variance) if v >= 0.95)
print(f"Retaining {n_components} principal components, which explain {cumulative_variance[n_components]*100:.2f}% of the variance.")

# Transform the data to the reduced dimensional space
data_pca = data_std.iloc[:, :n_components]

"""# Clustering

Implement K-means for clustering from scratch.
"""

def kmeans(X, n_clusters, max_iterations=100):
    """
    Implement the K-Means clustering algorithm from scratch.

    Parameters:
    X (np.ndarray): The input data to be clustered.
    n_clusters (int): The number of clusters to form.
    max_iterations (int): The maximum number of iterations to perform.

    Returns:
    np.ndarray: The cluster assignments for each data point.
    np.ndarray: The final cluster centroids.
    """
    # Initialize cluster centroids randomly
    centroids = X[np.random.choice(X.shape[0], n_clusters, replace=False)]

    for _ in range(max_iterations):
        # Assign data points to clusters
        distances = np.linalg.norm(X[:, None] - centroids, axis=-1)
        cluster_assignments = np.argmin(distances, axis=1)

        # Update cluster centroids
        new_centroids = np.array([X[cluster_assignments == i].mean(axis=0) for i in range(n_clusters)])

        # Check for convergence
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids

    return cluster_assignments, centroids

"""Using the function you've created to execute the K-means algorithm eight times on your data, with the number of clusters ranging from 2 to 9. For each run, display the genre of each cluster using the first two principal components in a plot."""

# Convert categorical columns to numerical format
data['track_artist'] = data['track_artist'].astype(str)
data['playlist_genre'] = data['playlist_genre'].astype(str)
data['playlist_subgenre'] = data['playlist_subgenre'].astype(str)

# Standardize the numerical columns
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data.drop(['track_artist', 'playlist_genre', 'playlist_subgenre'], axis=1))

# Perform PCA and select the first two principal components
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
pca_columns = [f'Principal Component {i+1}' for i in range(X_pca.shape[1])]

# Get the principal component vectors
pc1 = pca.components_[0]
pc2 = pca.components_[1]

# Find the top features for each principal component
top_pc1_features = data.columns[np.argsort(np.abs(pc1))[-3:]]
top_pc2_features = data.columns[np.argsort(np.abs(pc2))[-3:]]

print(f"Top features for Principal Component 1: {', '.join(top_pc1_features)}")
print(f"Top features for Principal Component 2: {', '.join(top_pc2_features)}")

# Run K-Means with different numbers of clusters
for n_clusters in range(2, 10):
    cluster_assignments, centroids = kmeans(X_pca, n_clusters)

    # Plot the clusters
    plt.figure(figsize=(8, 6))
    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_assignments, cmap='viridis')
    plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=200, c='red', label='Centroids')
    plt.title(f'K-Means Clustering with {n_clusters} Clusters')
    plt.xlabel(pca_columns[0])
    plt.ylabel(pca_columns[1])
    plt.legend()
    plt.show()
# Find the top features for each principal component
top_pc1_features = data.columns[np.argsort(np.abs(pc1))[-2:]]
top_pc2_features = data.columns[np.argsort(np.abs(pc2))[-2:]]

print(f"Top features for Principal Component 1: {', '.join(top_pc1_features)}")
print(f"Top features for Principal Component 2: {', '.join(top_pc2_features)}")

"""The Silhouette score and the Within-Cluster Sum of Squares (WSS) score are two metrics used to assess the quality of your clustering. You can find more information about these two methods [here](https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb). Plot the Silhouette score and the WSS score for varying numbers of clusters, and use these plots to determine the optimal number of clusters (k)."""

# Calculate Silhouette score and WSS for different numbers of clusters
silhouette_scores = []
wss_scores = []

for n_clusters in range(2, 11):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(X_scaled)

    # Calculate Silhouette score
    silhouette_score_value = silhouette_score(X_scaled, cluster_labels)
    silhouette_scores.append(silhouette_score_value)

    # Calculate Within-Cluster Sum of Squares (WSS)
    wss = kmeans.inertia_
    wss_scores.append(wss)

# Plot Silhouette score
plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), silhouette_scores)
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.title('Silhouette Score vs. Number of Clusters')
plt.show()

# Plot WSS
plt.figure(figsize=(8, 6))
plt.plot(range(2, 11), wss_scores)
plt.xlabel('Number of clusters')
plt.ylabel('WSS')
plt.title('Within-Cluster Sum of Squares (WSS) vs. Number of Clusters')
plt.show()

"""# Checking Output

To see how good was our clustering we will use a sample check and t-SNE method.

first randomly select two song from every cluster and see how close these two songs are.
"""

# Assuming you have the cluster assignments in a variable called 'cluster_assignments'
cluster_centers = []
for i in range(n_clusters):
    cluster_members = np.where(cluster_assignments == i)[0]
    if len(cluster_members) >= 2:
        sample_songs = random.sample(list(cluster_members), 2)
        print(f"Cluster {i}:")
        print(f"Song 1: {data.iloc[sample_songs[0]]}")
        print(f"Song 2: {data.iloc[sample_songs[1]]}")
        print(f"Distance between songs: {np.linalg.norm(X_scaled[sample_songs[0]] - X_scaled[sample_songs[1]])}")
        print()

"""Using t-SNE reduce dimension of data pointe to 2D and plot it to check how good datapoints are clustered (implementing this part is optional and have extra points)"""

# Compute the t-SNE embedding
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X_scaled)

# Plot the t-SNE embedding with cluster assignments
plt.figure(figsize=(8, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=cluster_assignments, cmap='viridis')
plt.title('t-SNE Visualization of Clusters')
plt.xlabel('t-SNE Dimension 1')
plt.ylabel('t-SNE Dimension 2')
plt.show()